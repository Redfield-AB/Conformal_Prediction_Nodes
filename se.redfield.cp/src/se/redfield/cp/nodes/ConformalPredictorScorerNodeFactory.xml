<?xml version="1.0" encoding="UTF-8"?>
<knimeNode icon="./icons/scorer.png" type="Other" xmlns="http://knime.org/node/v2.8" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://knime.org/node/v2.10 http://knime.org/node/v2.10.xsd">
    <name>Conformal Scorer</name>
    
    <shortDescription>
        Compares predictions made by Conformal Classifier with actual values.
    </shortDescription>
    
    <fullDescription>
        <intro>
        	Compares predictions made by Conformal Classifier with actual values. There are two types of scoring:
        	<ul>
        		<li>
        			Efficiency – the ratio of single label classification (right or wrong).
        			Calculated as <tt>Single class predictions / Total</tt><br/>
        			See <b>Additional prediction information</b> option section for parameters description.
        		</li>
        		<li>
        			Validity - counts the fraction of correct predictions.
        			If a record belongs to a mixed class containing the correct value it is considered to be correct.
        			Calculated as <tt>Total_match/Total</tt><br/>
        			See <b>Additional prediction information</b> option section for parameters description.
        		</li>
        	</ul>
        </intro>
        <option name="Target column">
        	A column that contains the real classes of the data.
        </option>
        <option name="Classes">
        	A column that contains predictions produced by Conformal Classifier. Could be both collection or string column type.
        </option>
        <option name="Additional prediction information">
        	Includes additional columns with some prediction metrics.
        	<ul>
        		<li>Exact match – number of correct predictions that belong to one class, and not belong to any mixed class.</li>
        		<li>Soft match - number of correct predictions that belong to one of the mixed classes.</li>
        		<li>Total match – Exact_match + Soft_match.</li>
        		<li>Error –  number of predictions that do not include the target class.</li>
        		<li>Total – total number of records that belongs to the current target class.</li>
        		<li>Single class predictions - number of predictions resulted in a single class</li>
        		<li>Null predictions - number of null predictions</li>
        	</ul>
        </option>
        <option name="Additional efficiency metrics">
        	Adds additional columns with efficiency metrics. The metrics are taken from the paper "Criteria of efficiency for set-valued classification" by Vovk et al. 
        	<ul>
        		<li>The S (“sum”) criterion measures efficiency by the average of the sum of p-values. Smaller values are preferable.</li>
        		<li>The N (“Number”) criterion uses the average size of the prediction sets. Smaller values are preferable.</li>
        		<li>The U (“unconfidence”) criterion uses the average unconfidence over the test sequence, where the unconfidence for a test object is the second largest p-value. Smaller values are preferable.</li>
        		<li>The F (“fuzziness”) criterion uses the average fuzziness where the fuzziness for a test object is defined as the sum of all p-values apart from a largest one. Smaller values are preferable.</li>
        		<li>The M (“multiple”) criterion uses the percentage of objects in the test sequence for which the prediction set at the given significance level is multiple, i.e., contains more than one label. Smaller values are preferable.</li> 
        		<li>The E (“excess”) criterion uses the average (over the test sequence, as usual) amount the size of the prediction set exceeds 1. Smaller values are preferable.</li>
        		<li>The OU (“observed unconfidence”) criterion uses the average observed unconfidence over the test sequence, where the observed unconfidence for a test example is the largest p-value for the false labels. Smaller values are preferable for this test.</li>
        		<li>The OF (“observed fuzziness”) criterion uses the average sum of the p-values for the false labels, smaller values are preferable.</li>
        		<li>The OM (“observed multiple”) criterion uses the percentage of observed multiple predictions in the test sequence, where an observed multiple prediction is defined to be a prediction set including a false label. Smaller values are preferable.</li>
        		<li>The OE (“observed excess”) criterion uses the average number of false labels included in the prediction sets at the given significance level; smaller values are preferable.</li>
        	</ul>
        </option>
    </fullDescription>
    
    <ports>
        <inPort index="0" name="Input">Table with ranked predictions and classes.</inPort>
        
        <outPort index="0" name="Results">The accuracy statistics table</outPort>
        
    </ports>    
</knimeNode>
